{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CommonFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_layers=2, embedding_size=384, num_heads=4, dropout_rate=0.2):\n",
    "        super(CommonFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Forward Transformer Encoder\n",
    "        self.forward_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, dropout=dropout_rate)\n",
    "        self.forward_transformer_encoder = nn.TransformerEncoder(self.forward_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Backward Transformer Encoder\n",
    "        self.backward_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, dropout=dropout_rate)\n",
    "        self.backward_transformer_encoder = nn.TransformerEncoder(self.backward_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Permute to (sequence_length, batch_size, embedding_size)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Forward pass\n",
    "        forward_output = self.forward_transformer_encoder(x)\n",
    "\n",
    "        # Backward pass\n",
    "        backward_output = self.backward_transformer_encoder(torch.flip(x, [0]))\n",
    "\n",
    "        # Concatenate forward and backward outputs along the feature dimension\n",
    "        concatenated_output = torch.cat([forward_output, torch.flip(backward_output, [0])], dim=-1)\n",
    "\n",
    "        # Permute back to (batch_size, sequence_length, embedding_size * 2)\n",
    "        concatenated_output = concatenated_output.permute(1, 0, 2)\n",
    "\n",
    "        return concatenated_output\n",
    "\n",
    "\n",
    "class DomainSpecificFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_layers=2, embedding_size=384, num_heads=4, dropout_rate=0.2):\n",
    "        super(DomainSpecificFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Forward Transformer Encoder\n",
    "        self.forward_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, dropout=dropout_rate)\n",
    "        self.forward_transformer_encoder = nn.TransformerEncoder(self.forward_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Backward Transformer Encoder\n",
    "        self.backward_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_size, nhead=num_heads, dropout=dropout_rate)\n",
    "        self.backward_transformer_encoder = nn.TransformerEncoder(self.backward_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Permute to (sequence_length, batch_size, embedding_size)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Forward pass\n",
    "        forward_output = self.forward_transformer_encoder(x)\n",
    "\n",
    "        # Backward pass\n",
    "        backward_output = self.backward_transformer_encoder(torch.flip(x, [0]))\n",
    "\n",
    "        # Concatenate forward and backward outputs along the feature dimension\n",
    "        concatenated_output = torch.cat([forward_output, torch.flip(backward_output, [0])], dim=-1)\n",
    "\n",
    "        # Permute back to (batch_size, sequence_length, embedding_size * 2)\n",
    "        concatenated_output = concatenated_output.permute(1, 0, 2)\n",
    "\n",
    "        return concatenated_output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DomainSpecificClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DomainSpecificClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.fc(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CADA(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_domains):\n",
    "        super(CADA, self).__init__()\n",
    "        self.common_feature_extractor = CommonFeatureExtractor(input_size, hidden_size)\n",
    "        self.domain_specific_feature_extractors = nn.ModuleList([DomainSpecificFeatureExtractor(input_size, hidden_size) for _ in range(num_domains)])\n",
    "        self.domain_specific_classifiers = nn.ModuleList([DomainSpecificClassifier(hidden_size, num_classes) for _ in range(num_domains)])\n",
    "\n",
    "    def forward(self, x_sources, x_target):\n",
    "        common_features = self.common_feature_extractor(x_sources)  # Common feature extraction for source domains\n",
    "        domain_specific_features = [domain_specific_feature_extractor(x_sources) for domain_specific_feature_extractor in self.domain_specific_feature_extractors]  # Domain-specific feature extraction for each source domain\n",
    "        domain_outputs = [domain_specific_classifier(domain_specific_feature) for domain_specific_feature, domain_specific_classifier in zip(domain_specific_features, self.domain_specific_classifiers)]  # Domain-specific classification for each source domain\n",
    "        # Domain alignment and classification for target domain (similar to source domains)\n",
    "        target_common_features = self.common_feature_extractor(x_target)\n",
    "        target_domain_specific_features = [domain_specific_feature_extractor(x_target) for domain_specific_feature_extractor in self.domain_specific_feature_extractors]\n",
    "        target_domain_outputs = [domain_specific_classifier(domain_specific_feature) for domain_specific_feature, domain_specific_classifier in zip(target_domain_specific_features, self.domain_specific_classifiers)]\n",
    "        return domain_outputs, target_domain_outputs\n",
    "\n",
    "def MMD_loss(source_features, target_features):\n",
    "    # Calculate MMD loss\n",
    "    # This function calculates the Maximum Mean Discrepancy (MMD) loss between source and target features\n",
    "    return torch.mean(torch.pow(torch.mean(source_features, dim=0) - torch.mean(target_features, dim=0), 2))\n",
    "\n",
    "def classifier_discrepancy_loss(domain_outputs):\n",
    "    # Calculate classifier discrepancy loss\n",
    "    # This function calculates the discrepancy loss between domain-specific classifiers\n",
    "    num_domains = len(domain_outputs)\n",
    "    loss = 0\n",
    "    for i in range(num_domains - 1):\n",
    "        for j in range(i + 1, num_domains):\n",
    "            loss += torch.mean(torch.abs(domain_outputs[i] - domain_outputs[j]))\n",
    "    return loss / (num_domains * (num_domains - 1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(model, source_loaders, target_loader, optimizer, num_epochs, alpha, beta):\n",
    "    model.train()\n",
    "    num_domains = len(source_loaders)\n",
    "    for epoch in range(num_epochs):\n",
    "        for domains_data in zip(*source_loaders, target_loader):\n",
    "            optimizer.zero_grad()\n",
    "            source_domain_outputs = []\n",
    "            for source_loader in source_loaders:\n",
    "                x_sources, _ = next(iter(source_loader))  # 从每个源领域加载器中获取数据\n",
    "                source_domain_outputs.append(model.common_feature_extractor(x_sources))  # 提取每个源领域的特征\n",
    "            target_data, _ = next(iter(target_loader))  # 从目标领域加载器中获取数据\n",
    "            target_domain_outputs = model.common_feature_extractor(target_data)  # 提取目标领域的特征\n",
    "            domain_outputs = source_domain_outputs + [target_domain_outputs]  # 汇总所有领域的特征\n",
    "            mmd_loss = MMD_loss(torch.cat([domain_output for domain_output in domain_outputs[:-1]]), domain_outputs[-1])  # 计算 MMD 损失\n",
    "            disc_loss = classifier_discrepancy_loss(domain_outputs)  # 计算分类器差异损失\n",
    "            total_loss = sum([domain_output_loss for domain_output_loss in domain_outputs[:-1]]) + alpha * mmd_loss + beta * disc_loss  # 总损失\n",
    "            total_loss.backward()\n",
    "            optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs, _ = model(inputs, None)  # 注意：这里传入的目标数据为 None，因为在测试阶段我们只关注源域数据的输出\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Test Accuracy: {:.2f}%'.format(accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数据集相关参数\n",
    "input_size = 384  # 输入特征的大小\n",
    "hidden_size = 1280  # 隐藏层大小\n",
    "num_classes = 2  # 类别数量\n",
    "num_domains = 2  # 源域数量\n",
    "batch_size = 128  # 批量大小\n",
    "\n",
    "\n",
    "num_layers = 2  # Transformer 层数\n",
    "num_heads = 4  # Transformer 注意力头数\n",
    "dropout_rate = 0.2  # Dropout 概率\n",
    "\n",
    "# 优化器相关参数\n",
    "learning_rate = 0.001  # 学习率\n",
    "alpha = 0.1  # MMD 损失的权重\n",
    "beta = 0.1  # 分类器差异损失的权重\n",
    "\n",
    "num_epochs = 10  # 训练轮数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_portion = 0.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D1_normal_logs, D1_abnormal_logs = load_data('BGL', portion=train_portion)\n",
    "D2_normal_logs, D2_abnormal_logs = load_data('Spirit', portion=train_portion)\n",
    "D3_normal_logs, D3_abnormal_logs = load_data('TB', portion=train_portion)\n",
    "\n",
    "# 创建数据集\n",
    "dataset_A = LogDataset(D1_normal_logs, D1_abnormal_logs)\n",
    "dataset_B = LogDataset(D2_normal_logs, D2_abnormal_logs)\n",
    "dataset_C = LogDataset(D3_normal_logs, D3_abnormal_logs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_size_A = int(train_portion * len(dataset_A))\n",
    "test_size_A = len(dataset_A) - train_size_A\n",
    "train_dataset_A, test_dataset_A = random_split(dataset_A, [train_size_A, test_size_A])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_size_B = int(train_portion * len(dataset_Bb))\n",
    "test_size_B = len(dataset_B) - train_size_B\n",
    "train_dataset_B, test_dataset_B = random_split(dataset_B, [train_size_B, test_size_B])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_size_C = int(train_portion * len(dataset_C))\n",
    "test_size_C = len(dataset_C) - train_size_C\n",
    "train_dataset_C, test_dataset_C = random_split(dataset_C, [train_size_C, test_size_C])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "source_loaders = [DataLoader(dataset_A, batch_size=batch_size, shuffle=True),\n",
    "                  DataLoader(dataset_B, batch_size=batch_size, shuffle=True)]\n",
    "\n",
    "target_loader = DataLoader(test_dataset_C, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_C, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CADA(input_size, hidden_size, num_classes, num_domains)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "train(model, source_loader, target_loader, optimizer, num_epochs, alpha, beta)\n",
    "\n",
    "# 在测试集上评估模型\n",
    "test(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
